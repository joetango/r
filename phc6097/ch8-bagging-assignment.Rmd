---
title: 'Chapter 8 Assignment: Bagging, RF, BART'
author: "Joe Dickerson"
date: "2025-10-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}

library(ISLR2)
library(randomForest)
library(BART)

data(Carseats)
set.seed(1)

```

Apply boosting, bagging, random forests, and BART to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like 
linear or logistic regression? Which of these approaches yields the best performance?

## Bagging

When we use bagging, we get an MSE of 2.624. 

```{r}

train <- sample(1:nrow(Carseats), (nrow(Carseats)/2))
test <- Carseats[-train, ]

bag.Carseats <- randomForest(Sales ~ ., data = Carseats, subset = train,
                         mtry = (ncol(Carseats) - 1), importance = TRUE)

bag.Carseats

Carseats.test <- Carseats[-train, "Sales"]

yhat.bag <- predict(bag.Carseats, newdata = test)
plot(yhat.bag, test[,"Sales"])
abline(0, 1)

mean((yhat.bag - test[,"Sales"])^2) ## test set MSE: 2.624

```

## Random Forest

When we using random forests, we get an MSE of 3.001. 

```{r}

rf.Carseats <- randomForest(Sales ~ ., data = Carseats, subset = train,
                            mtry = round(sqrt(ncol(Carseats))),
                            importance = TRUE)
yhat.rf <- predict(rf.Carseats, newdata = test)

mean((yhat.rf - test[,"Sales"])^2) ## test set MSE: 3.001

plot(yhat.rf, test[,"Sales"])
abline(0, 1)

```

## BART

When we use BART, we get an MSE of 1.438. Of the three methods, this is the most effective so far.

```{r}

x <- Carseats[, 2:11]
y <- Carseats[, "Sales"]
xtrain <- x[train, ]
ytrain <- y[train]

xtest <- x[-train, ]
ytest <- y[-train]

bartfit <- gbart(xtrain, ytrain, x.test = xtest)

yhat.bart <- bartfit$yhat.test.mean

mean((ytest - yhat.bart)^2) ## test set MSE: 1.438

ord <- order(bartfit$varcount.mean, decreasing = T)
bartfit$varcount.mean[ord] 

## checking how many times each variable appeared in the collection of trees
                          
```

## Linear Model

Strangely, a simple linear model seems to fit this data the best of the methods, just judging by the MSE alone, which for the linear model is 1.007, the lowest of all the methods.

```{r}

lmod <- lm(Sales ~ ., data = Carseats)
summary(lmod)

lm.preds <- predict(lmod)

mean((Carseats$Sales - lm.preds)^2) ## MSE: 1.007

```

## Conclusion

It appears that the three approaches yield similar results, with BART the most effective of the bunch utilizing trees. However, for this particular data set, it seems a simple linear model is the most effective. After looking at some descriptive plots of the data, it could be due to the high amount of noise in the data. 

```{r, warning=FALSE, message=FALSE}

library(tidyverse)
library(gridExtra)

p1 <- Carseats %>% 
  ggplot(aes(x = Age, y = Sales)) +
  geom_point(size = .5) +
  geom_smooth(fill = NA, method = lm)

p2 <- Carseats %>% 
  ggplot(aes(x = Income, y = Sales)) +
  geom_point(size = .5) +
  geom_smooth(fill = NA, method = lm)

p3 <- Carseats %>% 
  ggplot(aes(x = Price, y = Sales)) +
  geom_point(size = .5) +
  geom_smooth(fill = NA, method = lm)

p4 <- Carseats %>% 
  ggplot(aes(x = CompPrice, y = Sales)) +
  geom_point(size = .5) +
  geom_smooth(fill = NA, method = lm)

grid.arrange(p1, p2, p3, p4, ncol = 2)

```