---
title: "Chapter 3 Assignment"
author: "Joe Dickerson"
date: "2025-09-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Problem 9.

a.

```{r}

library("ISLR")
data(Auto, package="ISLR")

pairs(Auto)

```

b.

```{r}

cor(Auto[, -9])

```

c. 

i. There does appear to be a relationship between the predictors and the response, however not all predictors seem to have a relationship.

ii. Displacement, weight, year, and origin appear to have statistical significance.

iii. With all other predictors constant, we would expect the mpg to increase by 0.751 each additional year.

```{r}

lmod <- lm(mpg ~ . -name, data=Auto)
summary(lmod)

```

d. There appears to be some nonlinearity in our residuals vs fitted values graph. The Q-Q plot shows normally distributed residuals, though they may be skewed to the right. There is an outlier on the leverage plot, and it is labeled as observation 14. 

```{r}

par(mfrow = c(2,2))
plot(lmod)

```

e. The interaction term between weight and displacement appears statistically significant as it has a very small p-value. We see the same result when creating an interaction between weight and year, which suggests statistical significance. However, when we make an interaction between weight and year using :, which creates an interaction without the main effects, the interaction loses its statistical significance. 

```{r}

lmod2 <- lm(mpg ~ weight*displacement + year + origin + horsepower + cylinders + acceleration, data=Auto)

summary(lmod2)

lmod3 <- lm(mpg ~ weight*year + displacement + origin + horsepower + cylinders + acceleration, data=Auto)

summary(lmod3)

lmod4 <- lm(mpg ~ weight:year + displacement + origin + horsepower + cylinders + acceleration, data=Auto)

summary(lmod4)

```

f. We still see statistical significance when doing a log transformation on weight, and when squaring displacement. We also see statistical significance when taking the square root of year. 

```{r}

lmod5 <- lm(mpg ~ log(weight) + displacement^2 + year + origin + horsepower + cylinders + acceleration, data=Auto)

summary(lmod5)

lmod6 <- lm(mpg ~ weight + displacement + sqrt(year) + origin + horsepower + cylinders + acceleration, data=Auto)

summary(lmod6)

```


Problem 14.

a. 

The form of the linear model is a function of x1 and x2

  y = 2 + 2$x_1$ + 0.3$x_2$ + rnorm(100)

  The regression coefficients are:
  $B_0$ = -2
  $B_1$ = 2
  $B_2$ = 0.3

```{r}

set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)

```

b. We see a positive correlation between x1 and x2. The correlation found using the cor() function is 0.8351

```{r}

plot(x1, x2)
cor(x1, x2)

```

c.

$\hat{\beta}_0$ = 2.131

$\hat{\beta}_1$ = 1.440

$\hat{\beta}_2$ = 1.010

The beta-hat values are relatively similar to our beta values being around, and x1 has statistical significance, though x2 does not appear to. It appears we cannot reject either null hypothesis, $B_1$ or $B_2$ due to the lack of statistical significance. 

```{r}

lm1 <- lm(y ~ x1 + x2)

summary(lm1)

```

d. 

When we include only x1, the coefficient is even closer to our beta value of 2. We also see a much smaller p-value. We can reject the null hypothesis of B1 = 0.

```{r}

lm2 <- lm(y ~ x1)

summary(lm2)

```

e. When we only include x2 in the model, it becomes significant with a very small p-value. With this model alone, it appears the null hypothesis can be rejected. 

```{r}

lm3 <- lm(y ~ x2)

summary(lm3)

```

f. 

The results appear to contradict each other, as the first model cannot reject the null hypothesis while the following two can. It appears that there is some aspect of the predictors that create a lot of noise when observed together.

g. 

In the model that includes both predictors, x1 has a slightly higher p-value, and x2 has a significantly smaller p-value. It has become certainly statistically significant. In our models that include only one of the predictors, once again they are both statistically significant. With both of those models, the null hypothesis can be rejected. In the model with both predictors, per cook's distance, the new point appears to be a leverage point. It does not appear to be an outlier. In the model with just x1, it appears to be an outlier, though in the model with just x2, it does not appear to be so.

```{r}

x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)

lm4 <- lm(y ~ x1 + x2)

summary(lm4)

lm5 <- lm(y ~ x1)

summary(lm5)

lm6 <- lm(y ~ x2)

summary(lm6)

par(mfrow = c(2,2))
plot(lm4)

par(mfrow = c(2,2))
plot(lm5)

par(mfrow = c(2,2))
plot(lm6)

```